\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{pst-node,pst-tree,pstricks}
\usepackage{amssymb,amsmath}
\usepackage{hyperref}
\usepackage{pst-node}
\usepackage{mathtools}
\usepackage{amsthm}

% environments shortcuts
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqa}{\begin{eqnarray}}
\newcommand{\eeqa}{\end{eqnarray}}
\newcommand{\beqas}{\begin{eqnarray*}}
\newcommand{\eeqas}{\end{eqnarray*}}
\newcommand{\codim}{\text{codim}}

\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\bits}{\begin{itemize*}}
\newcommand{\eits}{\end{itemize*}}
\newenvironment{enumerate*}{\begin{enumerate}
    \setlength{\topsep}{0ex}
    \setlength{\parskip}{0ex}
    \setlength{\partopsep}{-1ex}
    \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0ex}}
{\end{enumerate}}

\newcommand{\benum}{\begin{enumerate*}}
\newcommand{\eenum}{\end{enumerate*}}
%\newcommand{\benums}{\begin{enumerate*}}
%\newcommand{\eenums}{\end{enumerate*}}
\newcommand{\mybullet}{$\bullet$}

% math mode commands

\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\rrr}{{\mathbb R}}
\newcommand{\bigOO}{{\cal O}}
\newcommand{\dataset}{{\cal D}}

\newcommand{\X}{\mathbf{X}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\ind}{\text{Ind}}
\newcommand{\res}{\text{Res}}
\newcommand{\vol}{\text{Vol}}

\newcommand{\trace}{\operatorname{trace}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\sign}{\operatorname{sgn}}
\newcommand{\onevector}{{\mathbf 1}}
\newcommand{\bbone}[1]{{\mathbf 1}_{[#1]}}

\newcommand {\argmax}[2]{\mbox{\raisebox{-1.7ex}{$\stackrel{\textstyle{\rm #1}}{\scriptstyle #2}$}}\,}  % to replace with the amsmath construction

\newlength{\picwi}
\newcommand{\backskip}{\hspace{-2.5em}} % how much to skip back for an empty item?

% Set up some colors
\definecolor{myblue}{rgb}{0.14,0.11,0.49}
\definecolor{myred}{rgb}{0.74,0.1,0.05}
\definecolor{mygreen}{rgb}{0.,0.52,0.32}
\definecolor{myyellow}{rgb}{0.96,0.92,0.13}
\definecolor{myorange}{rgb}{0.7,0.41,0.1}
\definecolor{mypurple}{rgb}{0.51,0.02,.8}
\definecolor{mygray}{rgb}{0.6,0.6,0.6}

\newcommand{\myblue}[1]{\textcolor{myblue}{#1}}
\newcommand{\myred}[1]{\textcolor{myred}{#1}}
\newcommand{\mygreen}[1]{\textcolor{mygreen}{#1}}
\newcommand{\myorange}[1]{\textcolor{myorange}{#1}}
\newcommand{\myyellow}[1]{\textcolor{myellow}{#1}}
\newcommand{\mypurple}[1]{\textcolor{mypurple}{#1}}
\newcommand{\mygray}[1]{\textcolor{mygray}{#1}}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]

% Stlyle stuff
% notes are for students , \notes with \mmp{} are for me

\newcommand{\comment}[1]{}
\newcommand{\mmp}[1]{\emph MMP: {#1}}
\newcommand{\mydef}[1]{\myred{\bf {#1}}}
\newcommand{\myemph}[1]{\mygreen{ {#1}}}
\newcommand{\mycode}[1]{\myblue{\tt {#1}}}
\newcommand{\myexe}[1]{{\small \mypurple{Exercise} {#1}}}

\newcommand{\reading}[2]{{\small \myemph{{\bf Reading} CRLS:} {#1}, \myemph{Python APPB4AWD} {#2}}}


\begin{document}
\begin{Large}
\centerline{Math 285}
\centerline{Lecture Notes}  % lecture number here
\centerline{\bf }       % lecture title here
\centerline{}      %date here
\end{Large}

\section{September 6th}

We will start with a review of calculus, recast in into differential forms.

\begin{definition}
Let $U \in \mathbb{R}^n$ be an open set. For a function $f: U \to \mathbb{R}$, we say $f \in C^k_p$ at a point $p$ if all partial derivatives of $f$ with order $\leq k$ exist and are continuous at $k$.
\end{definition}

Example: $C^0(\mathbb{R})$ describes functions that are at least continuous over the real numbers.
In our setting, we will usually concern ourselves with functions that belong to $C^{\infty}$, where $C^\infty = \cap_{i=0}^\infty C^{i}$

\begin{definition}
Let $U \subseteq \mathbb{R}^n$, and let $f: U \to \mathbb{R}$. We call $f$ analytic at a point $p \in U$ if it agrees with its Taylor’s series at $p$ in some neighborhood of $p$.
\end{definition}

We notice that because taking derivatives is linear, that is, we can differentiate term by term, that if $f$ is analytic, then $f \in C^\infty$. However, the converse need not be true:

Consider:

$$ f = \begin{cases} e^{-1/x} & \text{ when } x > 0  \\ 0 & \text{ else } \end{cases} $$

Without too much work, we see that this function is continuous. Moreover, the derivative of $e^{-1/x}$ is equal to $x^{-2}e^{-1/x} = x^{-2}f$. Taking the limit as $x \to 0$, and using L’H\^opital’s rule where necessary, we can see this goes to 0. Alternatively, we can look at:

$$ f’(0) = \lim_{x \to 0} \frac{f(x) - f(0)}{x}$$

with reasonable usage of L’H\^opital’s.

The upshot is that, inductively, we may show that $f^{(k)}(0) = 0$, and thus, at the point $x = 0$, the Taylor series for $f$ is identically 0. However, in no neighborhood of 0, is $f(x)$ identically 0. Thus, $f$ is not analytic. However, via computation, we see that $f \in C^{\infty}$. So, $C^\infty \nRightarrow$ analytic.

Another way to see this concept, is if we think about Taylor’s Series up to $k$-th order. This is just a Taylor series truncated at the $k$-th term, with a remainder term $R_{k+1}$. Then, in such a view, $f$ is analytic at a point $p \iff \lim_{k \to \infty} R_k = 0$.

\begin{definition}
Let $U \in \mathbb{R}^n$ be a set, and $p \in U$. We call $U$ star-shaped with respect to $p$ if, for all $q \in U$, that the line segment $\overline{pq} \subset U$.
\end{definition}

This motivates the hypotheses for Taylor’s Theorem with a remainder term:

\begin{theorem}
Let $U \subset \mathbb{R}^n$ be a star-shaped open set with respect to a point $p \in U$. Let $f: U \to \mathbb{R}$. If $f \in C^\infty$, then there exist $g_1,...,g_n \in C^\infty$ such that

$$ f(x) = f(p) + \sum_{i=1}^n g_i(x) (x^i - p^i) \text{ with } g_i(p) = \frac{\partial f}{\partial x^i}(p) $$
\end{theorem}

Proof: Let $y \in U$, and let $x \in \overline{py} \subseteq U$. Taking a parametrization of $\overline{py}: x(t) = p + t(y-p) $ where the $i$-th component is given by $x^i(t) = p^i + t(y^i - p^i)$.

Now, consider $f(y) - f(p) = f(x(1)) - f(x(0))$. Using the fundamental theorem:

$$ f(x(1)) - f(x(0)) = \int_0^1 \frac{d}{dt} f(x(t)) dt  = \int_0^1 \sum \frac{\partial f}{\partial x^i} (x(t)) \frac{dx^i}{dt} dt = $$

$$ \sum_i  \int_0^1 \frac{\partial f}{\partial x^i} (p + t(y-p)) (y^i - p^i) dt =   \sum_i  \left(\int_0^1 \frac{\partial f}{\partial x^i} (p + t(y-p))dt \right) (y^i - p^i) $$

Thus, we identify: $$g_i(y) = \int_0^1 \frac{\partial f}{\partial x^i} (p + t(y-p))dt$$

It should be clear that:

$$g_i(p) =  \int_0^1 \frac{\partial f}{\partial x^i} (p + t(p-p))dt = \int_0^1 \frac{\partial f}{\partial x^i}(p)dt =  \frac{\partial f}{\partial x^i}(p) $$

as $ \frac{\partial f}{\partial x^i}(p)$ is not a function of $t$.

Further, by an application of the dominated convergence theorem:

$$ \frac{\partial}{\partial y^j} g_i(y) = \frac{\partial}{\partial y^j} \left( \int_0^1 \frac{\partial f}{\partial x^i} (p + t(p-p))dt\right) = $$
$$ \int_0^1 \frac{\partial}{\partial y^j} \frac{\partial f}{\partial x^i} (p + t(p-p))dt $$

which exists and is continuous because $f \in C^\infty$. Thus, $g_i \in C^\infty$

\section{September 11th}

We want to reformulate the concept of a tangent vector in a coordinate-free way, because we should not need to immerse our manifold in an ambient Euclidean space.

Note that we will use parentheses for a point, and angle brackets for a vectors.

Recall that for a surface traced out in $\mathbb{R}^3$ by some function $M: f(x^1, x^2, x^3) = 0$, we can say that the tangent space is:

$$ T_p(M)  = \{ v_p \in T_p(\mathbb{R}^3) : \nabla f(p) \cdot v_p = 0 \} $$

But this depends on the space we’re immersed in.

To move towards a coordinate independent description, we instead look at the directional derivative.

\begin{definition}
If $v_p \in T_p(U)$ and $f \in C^\infty(U)$, then the directional derivative of $f$ in the direction of $v_p$ at the point $p$ is denoted by $D_{v_p} f$.

Explicitly, we can describe this as a cross section $f(p + tv)$, and thus:

$$ D_{v_p} = \frac{d}{dt} \bigg|_{t=0} = \sum_i \frac{\partial }{\partial x^i}\bigg|_{x = (p + tv_p)} \frac{dx^i}{dt} \bigg|_{t=0} = \sum_i \frac{\partial }{\partial x^i}(p) v^i= \sum_i v_i \frac{\partial}{\partial x^i} \bigg|_{p}  $$ 
\end{definition}

This leads to the concept of the germs of a function:

\begin{definition}
Let $(f, U)$ denote a $C^\infty$ function and its domain: $f: U \to \mathbb{R}$. Fix a $p \in U$.

We say that $(f, U) \sim (g, V)$ if there exists $W \subset U \cap V$ such that $p \in W$, and that restricted to $W$, $f = g$. We denote these equivalence classes as $[(f,U)]$ and call these the germs of functions.

Further, we denote the set of equivalence classes at $p$ as $C_p^\infty$.

\end{definition}

With some work, we can show that due to our equivalence being on some neighborhood of $p$, and the derivative being a local characteristic, that we may apply the directional derivative as:

$$ D_{v_p}: C^\infty_p \to \mathbb{R}$$

Without too much trouble, we can see that there is a algebra of germs over $\mathbb{R}$ with the following operations:

$$ \begin{cases} [(f,U)] + [(g,V)] = [(f+g, U \cap V)] \\ [(f,U)] * [(g,V)] = [(f*g, U \cap V)] \\ \lambda[(f,U)] = [(\lambda f, U )] \end{cases} $$

\begin{proposition}
Let $D_{v_p} : C_p^\infty \to \mathbb{R}$.

(i) $D_{v_p}$ is $\mathbb{R}$-linear.

(ii) $D_{v_p}$ follows a Leibniz rule, that is: $D_{v_p}(fg) = D_{v_p}(f) g(p) + f(p) D_{v_p}(g) $


\end{proposition}

\begin{definition}
Let $D: C^\infty_p \to \mathbb{R}$. If $D$ satisfies (i) and (ii) from Proposition 2.1, then we call it a derivation at $p$, or equivalently, a point-derivation of $C^\infty_p$. 
\end{definition}

\begin{definition}
We denote the set of point-derivations of $C_p^\infty$ as $\mathcal{D}_p(\mathbb{R}^n)$.
\end{definition}

Note that $\mathcal{D}_p(\mathbb{R}^n)$ is closed under addition and scalar multiplication, but not under multiplication. Thus, this forms a vector space, but not an algebra.

So, now we can recast our tangent space.

\begin{theorem}

The map defined by: 

$$ \phi: T_p(\mathbb{R}^n) \to \mathcal{D}_p(\mathbb{R}) $$

such that $v_p \mapsto D_{v_p}$

is a linear isomorphism of vector spaces.

\end{theorem}

\begin{proof}

Suppose $D_{v_p}$ is the 0 operator. By definition:

$$ \varphi(v_p) = D_{v_p} = \sum_{i} v^i \frac{\partial}{\partial x^i} \bigg|_{p} $$

Since this is true for all functions, it is in particular true for the function $f = x^j$. Of course then:

$$ D_{v_p} (f) = \sum_{i} v^i \frac{\partial}{\partial x^i} \bigg|_{p} (f) = v^j_p = 0 $$

Since the choice of $x^j$ was arbitrary, this may be performed for each $x^j$. Thus, $v^i_p = 0$ for all $i$, and thus $v_p = 0$.

Now, let $D \in \mathcal{D}_p(\mathbb{R}^n)$ be an arbitrary point-derivation.

Define $D_{v_p} = \sum v^i \frac{\partial}{\partial x^i} \bigg|_p$ where $v^j =  D(x^j)$.

We claim that for an arbitrary $f \in C^\infty_p$, that $D f = D_{v_p} f$.

Using Theorem 1.1 (Taylor’s theorem with Remainder), we expand $f$ as:

$$ f(x) = f(p) + \sum g_i(x) (x^i - p^i) : g_i(p) =  \frac{\partial f}{\partial x^i}(p) $$

So, computing:

$$ D(f)  = D(f(p) + \sum g_i(x) (x^i - p^i)) = 0 + \sum D(g_i(x) (x^i - p^i)) = $$

$$\sum D(g_i) (p^i - p^i) + g_i(p) D(x^i - p^i) = \sum \frac{\partial f}{\partial x^i}(p) D(x^i) $$

We notice that this is exactly the same form as $\sum v^i \frac{\partial}{\partial x^i} \bigg|_p$ due to our identification of $v^i = D(x^i)$. Thus, $D = D_{v_p}$ on all $f$.
\end{proof}

Note: we will notate in the future as $e_{i,p} = \frac{\partial}{\partial x^i} \bigg|_p$ from now on. 

Because we have a bijection, we can establish the following definition:

\begin{definition}
$T_p(U)$ is the set of point derivations of $C^\infty_p$.
\end{definition}

\begin{definition}
Let $X: U \to \coprod_{p \in U} T_p (U)$. If $X_p \in T_p(U)$, we call such a function a vector field, where we use $\coprod$ to remind ourselves that the tangent spaces are disjoint.
\end{definition}

Unpacking the definition a little bit, if $X_p \in T_p(U)$ then:

$$ X_p = \sum a^i(p) \frac{\partial}{\partial x^i} \bigg|_p$$

Denoting $a^i: U \to \mathbb{R}$ as $p \mapsto a^i(p)$, then we have that:

$$ X = \sum a^i \frac{\partial}{\partial x^i}$$

\begin{definition}
Let $X$ be a vector field as above. We say that $X \in C^\infty$ if each $a^i: U \to \mathbb{R}$ is $C^\infty$.
\end{definition}

Notation: The set of $C^\infty$ vector spaces on $U$ is an $\mathbb{R}$-vector space. We denote this by $\mathcal{X}(U)$.

\section{September 13th}

Multilinear algebra:

Recall some easy examples.

The dot product $\langle u, v \rangle$ is a bilinear function that takes $V \times V \to k$.

Dual spaces:

Note that we like this point of view because we can always multiply functions, but we may not admit a multiplication on vectors. Cross products need not exist.

\begin{definition}
A covector of a vector space $V$ is a linear function $f: V \to \mathbb{R}$.
\end{definition}

\begin{definition}
The dual space of $V$ is the set of all covectors of $V$. We denote this as $V^*$.
\end{definition}

\begin{theorem}
Let $V$ be a vector space with basis $\{ e_i \}_{I=1}^n$, and we have $\alpha^i \in V^*$, where $\alpha^i(e_j) = \delta_i^j$. Then, $\{ \alpha^i \}$ form a basis for $V^*$. We call this the dual basis to $\{ e_j \}$.
\end{theorem}

\begin{proof}
Suppose $\sum_{I=1}^n c_i \alpha^I = 0$. Since these are functions on $V$, consider their action on $e_j$. Then, of course, we find that $c_j = 0$. Repeating this argument, this tells us that $c_i = 0$ for all $I$. In a similar fashion, if we assume $f$ to be a functional, its action is completely determined on the basis vectors. Then, we may construct g such that $g = \sum_{I=1}^n d_i \alpha^I$. And we notice $f-g = 0$ everywhere, so $f = g$.
\end{proof}

\begin{corollary}
The dimension of the dual space is the dimension of the vector space.
\end{corollary}

\begin{definition}
Let $f$ is a function, k-linear on $V$. That is, a function from $V \times V ... \times V \to \mathbb{R}$, linear in each argument. We call this object a $k$-tensor on $V$. Further, we call $f$ symmetric if, for any permutation of the arguments, $f$ is constant. $$ f(v_{\sigma(1)}, ..., v_{\sigma(n)}) = f(v_1,...,v_n)$$ for any $\sigma \in S_n$. Call $f$ alternative if, instead:
 $$ f(v_{\sigma(1)}, ..., v_{\sigma(n)}) = \text{sgn}(\sigma)f(v_1,...,v_n)$$
\end{definition}

\begin{definition}
Let $\sigma \in S_n$. We say that: 

$$ \text{sgn}(\sigma) = \begin{cases} 1 \text{ if } \sigma \text{ can be expressed as a product of an even number of transpositions} \\ -1 \text{ if } \sigma \text{ can be expressed as a product of an odd number of transpositions} \end{cases} $$

Equivalently, we can count the inversions.
\end{definition}

It turns out, that the symmetric tensors will not be terribly interesting for this course, but the alternating ones will be.

\begin{definition}
Denote the $k$-tensors on $V$ as the set $L_k(V)$.
Denote the alternating $k$-tensors on $V$ as the set $A_k(V)$
\end{definition}

\begin{definition}
Let $f \in L_k(V), g \in L_l(V)$. Denote the tensor product of $f$ and $g$ as $f \otimes g \in L_{k+k}(V)$, where:

$$ f \otimes g (v_1,...,v_{k+l}) = f(v_1,...,v_k)g(v_{k+1},...,v_{k+l}) $$

Note that this is not commutative, but it is associative.
\end{definition}

\begin{definition}
Let $f \in A_k(V), g \in A_l(V)$. We denote the wedge product of $f$ and $g$ as $f \wedge g$, computed as:
$$ f \wedge g (v_1,...v_{k+l}) = \frac{1}{k!l!} \sum_{\sigma \in S_{k+l}} \text{sgn}(\sigma)
f(v_{\sigma(1)},...,v_{\sigma(k)})g(v_{\sigma(k+1)},...,v_{\sigma(k+l)}) $$
\end{definition}

\begin{definition}
Let $\sigma \in S_{k+l}$. If $\sigma(1) < ... < \sigma(k)$ and $\sigma(k+1) < ... < \sigma(k+l)$, we call $\sigma$ a $(k,l)$ shuffle. Note that we have $\binom{k+l}{k}$
\end{definition}

We notice that instead of summing over all permutations of $S_n$, we may simply define the wedge product over $(k,l)$ shuffles, and drop the fraction in front.

\begin{definition}
Let $\sigma\in S_k$, and let $f \in L_k(V)$. Then, we denote the permutation of the arguments as $\sigma f (v_1,..,v_k) = f(v_{\sigma(1)},...,v_{\sigma(k)})$.
\end{definition}

\begin{definition}
Let $f \in L_k(V)$. We call the function:
$$ A(f) = \sum_{\sigma \in S_k} \text{sgn} (\sigma) (\sigma f)$$

the alternator of $f$.

\end{definition}

\begin{theorem}
For $f \in L_k(V)$, $A(k)$ is alternating.
\end{theorem}

\begin{proof}
Let $\tau \in S_n$. Then, we have that:
$$ \tau A(f) = \tau \sum_{\sigma}\text{sgn} (\sigma) (\sigma f) = \sum_{\sigma} \text{sgn} (\sigma) (\tau \sigma f) = \sum_{\sigma} \text{sgn} (\sigma) (\tau \sigma f) = \text{sgn}(\tau) \sum_\sigma \text{sgn}(\tau \sigma) (\tau \sigma) f $$

But, we see that summing over $\sigma$ is equivalent to summing over $\tau \sigma$ because it’s just a translation of the group. So, this is equal to $\text{sgn}(\tau) A(f)$, and we’re done.
\end{proof}

Thus, we notice that we can also write the wedge product:

$$ f \otimes g = \frac{1}{k! l!} A(f \otimes g)$$

Remark: The wedge product has the following properties:
(i) $f \wedge g = (-1)^{d_f d_g} g \wedge f$ where $d_f$ is the degree of $f$ and same for $g$.
(ii) It is associative, $(f \wedge g) \wedge h = f \wedge(g \wedge h)$.
(iii) $f \wedge g \wedge h = \frac{1}{k! l! m!} A(f \otimes g \otimes h)$.
(iv) For $\alpha^i \in V^*$, $(\alpha^1 \wedge...\wedge \alpha^k) (v_1,...,v_k) =
\det[\alpha^i(v_j)]$

\section{September 18th}

Differential forms on $\mathbb{R}^n$

Recall that a covector is a linear map from $V \to \mathbb{R}$, 1-tensor, alternating 1-tensor.

Analogously then, a covector field on an open set $U \subseteq \mathbb{R}$ assigns a covector to each point $u \in U$.

An alternative name for a covector field is a 1-form. 

Differentials of $f$:

\begin{definition}
For $f \in C^\infty(U)$, define the 1-form $df$ on $U$ via, for $p \in U$:

$$(df)_p (X_p) = D_{X_p} f := X_p f $$

\end{definition}

\begin{example}
$$(dx^i)_p \left(\frac{\partial}{\partial x^j} \bigg|_p\right) = \frac{\partial}{\partial x^j} \bigg|_p x^i = \delta_j^i $$
\end{example}

We notice that $\left\{ \frac{\partial}{\partial x^i} \right\}$ is a basis, and the standard basis for $T_p(\mathbb{R}^n)$.

Therefore, $\{ dx^i \}$ is the dual basis for $(T_p \mathbb{R}^n)^v : = T_p^*(\mathbb{R}^n)$, the cotangent space at $p$. 

Then, of course, every covector $w_p$ at $p$ may be expressed in this basis as:

$$ w_p = \sum b_i(p) dx^i_p $$

Viewing $b_i$ as a function over $U$, then:

$$ w = \sum b_i dx^i $$

where $b_i : U \to \mathbb{R}$.

\begin{definition}
A 1-form $ w = \sum b_i dx^i $ is $C^\infty$ if all $b_i \in C^\infty$.
\end{definition}

\begin{example}

Consider $\{ x, y, z\} \subset \mathbb{R}^3$ as the standard coordinates. 

$dx$ is a 1-form on $\mathbb{R}^3$ such that

$$ dx\left( a\frac{\partial}{\partial x} + b\frac{\partial}{\partial y} + c\frac{\partial}{\partial z} \right) = a$$

that is, it extracts the $x$-coordinate of a vector.
\end{example}

If we consider extracting coordinates via the dual basis, it’s not hard to see that, for $f \in C^\infty$:

$$ df = \sum \frac{\partial f}{\partial x^i} dx^i $$

Extending to $k$ arguments, we have the following:

\begin{definition}
A $k$-form on $U$ is an alternating $k$-tensor $w_p$ defined at each point $p \in U$. 
\end{definition}

Recall the following theorem:

\begin{theorem}
If $\alpha^1,...,\alpha^n$ is a basis for $A_1(V)$, then a basis for $A_k(V)$ is $\alpha^{i_1} \wedge ... \wedge \alpha^{i_k}$ where $i_1 < .. < i_k$. 
\end{theorem}

\begin{example}
In $\mathbb{R}^3$:

A 0-form associates to each point a number, so is a linear function $f: U \to \mathbb{R}$. 

A 1-form looks like:

$$ w = f dx + gdy + h dz $$

A 2-form looks like:

$$ w = P dy \wedge dz + Q dz \wedge dx + R dx \wedge dy $$

A 3-form looks like:

$$ w = f dx \wedge dy \wedge dz $$

In general, we define a $k$-form on $U \subset \mathbb{R}^n$ as:

$$ w = \sum_{I} b_I dx^I$$ where $I$ is a multi-index such that $1 \leq i_1 < ... < i_k \leq n$.
\end{example} 

We can see that there exists a 1-1 correspondence between 1-forms/2-forms and vector fields.

We can also see that these exists a 1-1 correspondence between 0-forms/3-forms and functions.

Not too hard to see, we would just look at these as abstract vector fields.

Exterior Derivative:

Notationally, we will denote $\Omega^k(U)$ as the $C^\infty$ k-forms on $U$.

\begin{definition}
We define the exterior derivative as a map:

$$d: \Omega^k(U) \to \Omega^{k+1}(U)$$

where $d\left( \sum_{I} b_I dx^I \right) = \sum_I db_I \wedge dx^I = \sum_{I,j} \frac{\partial b_I}{\partial x^j} dx^j \wedge dx^I$
\end{definition}

\begin{proposition}
(i) $d$ is an antiderivation of degree $1$:

$$ d(\omega \wedge \tau) = (d\omega) \wedge \tau + (-1)^{\deg \omega} \omega \wedge d\tau $$

(ii) $d \cdot c = 0$.

(iii) If $f$ is a 0-form, then:

$$ df(X) = Xf $$
\end{proposition}

See proof in book.

Recall some basics from vector calculus.

The gradient of $f$ may be viewed as:

$$\nabla f = \begin{bmatrix} f_x \\ f_y \\ f_x  \end{bmatrix}$$

We say this corresponds to the following 1-form:

$$df = f_x dx + f_y dy + f_z dz $$

Similarly, for the curl:

$$ \nabla \times f = \begin{bmatrix} \frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \\ \frac{\partial}{\partial z} \end{bmatrix} \times \begin{bmatrix} P \\ Q \\ R \end{bmatrix} = \begin{bmatrix} R_y - Q_z \\ P_z - R_x \\ Q_x - P_y \end{bmatrix} $$

This corresponds to the following 2-form:

$$(R_y - Q_z) dy \wedge dz + (P_z - R_x) dz \wedge dx + (Q_x - P_y) dx \wedge dy = d(P dx + Q dy + R dz) $$

Without writing it out, we can look at the divergence of a vector field, and we notice that it corresponds to the following 2-form:

$$d(P dy \wedge dz + Q dz \wedge dx + R dx \wedge dy) $$

In particular then, we see that these are special cases of the exterior derivative. More generally, if we let $\mathfrak{X}(U)$ denote $C^\infty$ vector fields on $U$, we see that we have a sequence:

$$ C^\infty(U) \to_{\text{grad}} \mathfrak{X}(U) \to_{\text{curl}} \mathfrak{X}(U) \to_{\text{div}} C^\infty(U) $$

and we have a parallel sequence of isomorphic structures:

$$ \Omega^0(U) \to_d  \Omega^1(U) \to_d  \Omega^2(U) \to_d \Omega^3(U)$$

\begin{theorem}
$$d^2 = 0 \iff \text{curl}(\text{grad } f) = 0, \text{div}(\text{curl } F) = 0 $$
\end{theorem}

Similar, we can look at Green’s theorem as a statement on differential forms:

\begin{theorem}
The Generalized Stokes’ Theorem
$$ \int_{\partial D} \omega = \iint_D d\omega $$
\end{theorem}

\section{September 20}

Topological Manifolds:

\begin{definition}
We call a topological space $M$ locally Euclidean if, for every $p \in M$, there exists a neighborhood $p \in U$ such that $U$ is homeomorphic to a neighborhood of a Euclidean space $\phi: U \to \phi(U) \subseteq \mathbb{R}^n$, for some $n$. For such an $n$, we say that $M$ is locally Euclidean of dimension $n$. We call a pair $(U, \phi)$ a chart, or a coordinate neighborhood of $M$.
\end{definition}

\begin{definition}
If a topological space is locally Euclidean, Hausdorff, and second countable, we call it a topological manifold.
\end{definition}

Note that the idea here is that hope that topological manifolds can be embedded in some $\mathbb{R}^n$, and $\mathbb{R}^n$ is Hausdorff, second countable. Since topological subspaces remain Hausdorff, second countable, we restrict ourselves to the study of this class of manifolds.

\begin{example}
We want to show that the set $\{ (x,y) : xy = 0 \}$ is not locally Euclidean. Suppose it were around the origin $p = (0,0)$. Then, we note that we have a homeomorphism from $U \setminus p \to \phi(U \setminus p) \subseteq \mathbb{R}^n$. However, we notice that $U \setminus p$ has 4 connected components, but removing a single point from any Euclidean neighborhood gives us either 2 components in $\mathbb{R}$ and $1$ else. Thus, such a homeomorphism cannot exist.
\end{example}

\begin{definition}
Let $U, V$ be open sets, locally Euclidean with charts $\varphi: U \to \mathbb{R}^n, \psi: V \to \mathbb{R}^n$. We call $\psi, \varphi$  $C^\infty$ compatible if both:

$$\begin{cases} \psi \circ \varphi^{-1}: \varphi(U \cap V) \to \psi(U \cap V)\\  \varphi \circ \psi^{-1}: \psi(U \cap V) \to \varphi(U \cap V) \end{cases}$$

are $C^\infty$ maps.
\end{definition}

\begin{definition}
Let $\{ (U_\alpha, \phi_\alpha) \}_{\alpha \in I}$ be a collection of $C^\infty$ charts, such that $M = \cup_\alpha U_\alpha$. We call this collection an atlas.

\end{definition}

\begin{definition}
Let $\mathfrak{U}$ be a $C^\infty$ atlas. We call it maximal if it is not contained in any other $C^\infty $ atlas, that is, if $\mathfrak{U} \subseteq \mathfrak{M}$, then $\mathfrak{U} = \mathfrak{M}$.

\end{definition}

\begin{definition}
We call a topological manifold equipped with a maximal $C^\infty$ atlas, a $C^\infty$ manifold.
\end{definition}

\begin{theorem}
Every $C^\infty$ atlas on a locally Euclidean space is contained within a unique, maximal $C^\infty$ atlas.
\end{theorem}

\begin{theorem}
If $M, N$ are $C^\infty$ manifolds, so too is $M \times N$.

\end{theorem}

\section{September 25}

We want to define smooth maps, the morphisms in the category of topological manifolds.

First, let’s talk about smooth functions, that is, for $M$ a topological manifold, $f: M \to \mathbb{R}$.

\begin{definition}
Let $M$ be a topological manifold. We say that a map $f: M \to \mathbb{R}$ is $C^\infty$ at a point $p \in M$ if there exists a chart $(U, \phi)$ of $M$ about $p$ such that:

$$ f \circ \phi^{-1}: \phi(U) \to \mathbb{R} $$ is $C^\infty$ at $\phi(p)$.

\end{definition}

First, is this well-defined? Is this independent of the choice of chart?

Suppose that we have another chart about $p$: $(V, \psi)$. Of course, $p \in U \cap V$, so we may use the transition maps:

$$ f \circ \psi^{-1} = (f \circ \phi^{-1}) \circ ( \phi \circ \psi^{-1}) $$

We have a priori that $f \circ \phi^{-1}$, is $C^\infty$. And, because these charts belong to the same maximal atlas, they must be $C^\infty$ compatible, hence $ \phi \circ \psi^{-1}$ is $C^\infty$. And the composition of two $C^\infty$ functions is $C^\infty$. 

\subsection{Smooth maps}

First, a technical note:

\begin{theorem}
If $F: M \to N$ and $G: N \to P$ are $C^\infty$ maps, then $G \circ F$ is a $C^\infty$ map
\end{theorem}

\begin{definition}
Let $M, N$ be topological manifolds, and let $F: M \to N$.

We say that $F$ is $C^\infty$ at a point $p\in M$ if there exists charts $(U, \phi)$ of $p \in M$ and $(V, \psi)$ of $F(p)\in N$ such that $  \psi \circ F \circ \phi^{-1} $ is $C^\infty$ at $\phi(p)$.

\end{definition}

Without too much trouble, you can see that every coordinate map of a chart is $C^\infty$ by following the definition, and picking $\phi^{-1}$ to pull from $\mathbb{R}^n$, and taking the identity on the target $\mathbb{R}^n$. This also applies to the inverse of the coordinate map.

\begin{definition}
Let $F: M \to N$ be an bijective, $C^\infty$ map. If $F^{-1}$ is also $C^\infty$, then we call $F$ a diffeomorphism.

\end{definition}

\begin{example}
Let $(U, \phi)$ be a chart of $M$. Then, $\phi, \phi^{-1}$ are diffeomorphisms between $U, \phi(U) \subseteq \mathbb{R}^n$
\end{example}

\begin{theorem}
Let $F: U \to F(U)$ be a diffeomorphism from an open subset $U \subseteq M$ to a subset of $\mathbb{R}^n$ for some $n$. Then, $(U, F)$ defines a chart of $M$. In particular, this chart is compatible with the maximal atlas fixed by the choice of $M$. 

\end{theorem}

\begin{proof}
First, we want to see that $F$ is a homeomorphism. Without too much trouble, we look at $\psi \circ F \circ \phi^{-1}$, where $\psi$ is a chart for $\mathbb{R}^n$, and $\phi$ is a chart for $U$. Since this is a $C^\infty$ map, it is continuous, and since $\psi, \phi^{-1}$ are continuous, so too must be $F$. Therefore, $F$ is a homeomorphism, since we can play the same game with $F^{-1}$.

Next, suppose $(U_\alpha, \phi_\alpha)$ is a member of the maximal atlas on $M$. Well, since $F, \phi_\alpha$ are $C^\infty$ maps, we have that $F \circ \phi_\alpha^{-1}$ and $\phi_\alpha\circ F^{-1}$ are $C^\infty$. Thus, $(U, F)$ is $C^\infty$ compatible with $(U_\alpha, \phi_\alpha)$. Since the choice of $(U_\alpha, \phi_\alpha)$ were arbitrary, $(U,F)$ is compatible with every chart in your maximal atlas, and by the definition of the maximal atlas, it must belong to the maximal atlas.
\end{proof}

Therefore, since we have an association with diffeomorphisms and charts, we can say that each diffeomorphism gives rise to a choice of coordinate system.

\subsection{Partial Derivatives on a Chart}

Let $(U, \phi)$ be a chart of $M$. We define the partial derivatives on a chart as follows, where we define $x^i = r^i \circ \phi$.

\begin{definition}

$$ \frac{\partial f}{\partial x^i} (p) = \frac{\partial}{\partial x^i} \bigg|_p f = \frac{\partial f \circ \phi^{-1}}{\partial r^i} (\phi(p)) $$
\end{definition}

\begin{theorem}

$$ \frac{\partial x^i}{\partial x^j}(p) = \delta_j^i $$

\end{theorem}

\begin{proof}
Clear, by using the definition of $x^i$ as combined with the definition of partial derivatives.

\end{proof}

\subsection{Inverse Function Theorem}

\begin{definition}
We call a map $F: M \to N$ locally invertible at $p \in M$ (equivalently, a local diffeomorphism at $p$) if there exists $U$, a neighborhood of $p$ and $V$, a neighborhood of $F(p) \in N$ such that $F: U \to V$ is a diffeomorphism.
\end{definition}

\begin{theorem}
Let $F: W \subseteq \mathbb{R}^n \to \mathbb{R}^n$ be $C^\infty$ on an open set $W$. Then, $F$ is locally invertible at $p$ if and only if, for $r^i$ the standard coordinates on $\mathbb{R}^n$, the determinant of the matrix with $i,j$-th entry: $\partial F^i /\partial r^j$ does not vanish at $p$.
\end{theorem}

\begin{theorem}
Let $(U, \phi) = (U, (x^1...,x^n))$ be a chart of $M$ around a point $p$, and let $(V, \psi) = (V, (y^1,...,y^n))$ be a chart of $N$ around $F(p)$, where $F: U \to V$ is a $C^\infty$ map.

Define $F^i = y^i \circ F$, that is, the $i$-th component function of $F$. Then, we have that $F$ is locally invertible at $p \in U$ if and only if the determinant of the matrix with $i,j$-th entry: $\partial F^i /\partial x^j$ does not vanish at $p$.
\end{theorem}

\begin{proof}
Using the definition of partial derivatives on a chart, we can instead say then that $F$ is a local diffeomorphism at $p \in U \subseteq M$ if and only if $\psi \circ F \circ \phi^{-1}$ is a local diffeomorphism at $\phi(p) \in \mathbb{R}^n$ if and only if the matrix with entries:

$$\left[ \frac{\partial(\psi \circ F \circ \phi^{-1})^i}{\partial r^j} \right]_{i,j} $$

has non vanishing determinant at $\phi(p)$

However, we notice that:

$$(\psi \circ F \circ \phi^{-1})^i = \psi^i \circ F \circ \phi^{-1} = y^i \circ F \circ \phi^{-1} = F^i \circ \phi^{-1}$$

Thus, we recover the statement in the theorem.
\end{proof}

\section{September 27th}

So we can discuss some examples of $C^\infty$ manifolds by exhibiting an atlas explicitly. However, we have other ways to construct manifolds.

We think of this via gluing edges. The classic example is identifying opposite edges of a square to make a torus. But, how do we formalize this?

Start with the unit square as an example. First, define $(x,0) \sim (x,1)$ and $(0,y) \sim (1,y)$. From here, we wish to generate an equivalence relationship by enforcing the reflexivity, symmetric, and transitive properties. 

In general, suppose $\sim$ is an equivalence relationship on a topological space $S$. Denote $[x]$ as the equivalence class of $x$. Define the quotient of $S$ by $\sim$ as the set of equivalence classes $[x]$. We denote this quotient as $S/\sim$.

We may naturally endow this space with the quotient topology from $S$, that is, we say $U \subseteq S/\sim$ is open if, for the projection $\pi: S \to S/\sim$ which sends $x \mapsto [x]$, $\pi^{-1}(U)$ is open. Equivalently, we take the finest topology such that the projection is continuous.

Another example: Take $S = \mathbb{R}$, and take the equivalence relationship that only associates $0 \sim 1$. Then, of course, this is not a manifold because it is not locally Euclidean.

\subsection{A necessary condition for a quotient to be Hausdorff}

\begin{proposition}
Let $X$ be a Hausdorff topological space. Let $\{ x \} \subset X$ be any singleton set. Then, $\{ x \}$ is closed.
\end{proposition}

\begin{theorem}
If $S / \sim$ is Hausdorff, then for all $x \in S$, the equivalence class $[x] \subseteq S$ is a closed set in $S$.
\end{theorem}

So of course, now we can say when a quotient is not Hausdorff. However, we would like a sufficient condition as well.

\subsection{Open maps}

\begin{definition}
Let $f : X \to Y$ be a map between topological spaces. We call $f$ open if, for any open set $U \subset X$, $f(U)$ is open in $Y$.
\end{definition}

Non-example: $f: \mathbb{R} \to \mathbb{R}$ that sends $x \to x^2$ is not an open map.

\begin{definition}
Suppose $\pi: S \to S/\sim$, the projection, is an open map. Then, we call $\sim$ an open equivalence relation.
\end{definition}

\begin{definition}
If $\sim$ is an equivalence relation on $S$, define $R = \{ (x,y) \in S \times S : x \sim y \}$.
\end{definition}

\begin{theorem}
If $\pi: S \to S/\sim$ is an open map, and $R$ is closed in $S \times S$, then the quotient space $S/\sim$ is Hausdorff.
\end{theorem}

\begin{proof}
Let $[x] \not = [y] \in S/\sim$. Then, we must have that $x \not \sim y$ and therefore $(x,y) \not \in R$. Then, since $R$ is closed, we may find an open rectangle $U \times V$ that contains $(x,y)$ such that $(U \times V) \cap R$ is trivial.

Of course then, we have that $x \in U$, $y \in V$. By the condition that the intersection is trivial, we must have that for all $u \in U, v \in V$, $u \not \sim v$.

We may look then at $\pi(U), \pi(V)$. Of course, since $U, V$ are open sets in $S$, we must have that these are open sets in the quotient. Furthermore, of course $[x] \in \pi(U), [y] \in \pi(V)$. Lastly, their intersection must be trivial as if we may find a $[z] \in \pi(U) \cap \pi(V)$, then, we would have a $(z,z) \in R$. But that is a contradiction. Therefore, we can find disjoint neighborhoods of $[x], [y]$. Since $[x], [y]$ were arbitrary, this can be done for any $[x],[y]$, and therefore, $S/\sim$ is Hausdorff.
\end{proof}

\begin{corollary}
If the diagonal $\Delta$ is a closed set in $S \times S$, then $S$ is Hausdorff. 
\end{corollary}

In fact, the converse is also true.

\end{document}