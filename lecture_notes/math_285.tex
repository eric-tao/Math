\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{pst-node,pst-tree,pstricks}
\usepackage{amssymb,amsmath}
\usepackage{hyperref}
\usepackage{pst-node}
\usepackage{mathtools}
\usepackage{amsthm}

% environments shortcuts
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqa}{\begin{eqnarray}}
\newcommand{\eeqa}{\end{eqnarray}}
\newcommand{\beqas}{\begin{eqnarray*}}
\newcommand{\eeqas}{\end{eqnarray*}}
\newcommand{\codim}{\text{codim}}

\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\bits}{\begin{itemize*}}
\newcommand{\eits}{\end{itemize*}}
\newenvironment{enumerate*}{\begin{enumerate}
    \setlength{\topsep}{0ex}
    \setlength{\parskip}{0ex}
    \setlength{\partopsep}{-1ex}
    \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0ex}}
{\end{enumerate}}

\newcommand{\benum}{\begin{enumerate*}}
\newcommand{\eenum}{\end{enumerate*}}
%\newcommand{\benums}{\begin{enumerate*}}
%\newcommand{\eenums}{\end{enumerate*}}
\newcommand{\mybullet}{$\bullet$}

% math mode commands

\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\rrr}{{\mathbb R}}
\newcommand{\bigOO}{{\cal O}}
\newcommand{\dataset}{{\cal D}}

\newcommand{\X}{\mathbf{X}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\ind}{\text{Ind}}
\newcommand{\res}{\text{Res}}
\newcommand{\vol}{\text{Vol}}

\newcommand{\trace}{\operatorname{trace}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\sign}{\operatorname{sgn}}
\newcommand{\onevector}{{\mathbf 1}}
\newcommand{\bbone}[1]{{\mathbf 1}_{[#1]}}

\newcommand {\argmax}[2]{\mbox{\raisebox{-1.7ex}{$\stackrel{\textstyle{\rm #1}}{\scriptstyle #2}$}}\,}  % to replace with the amsmath construction

\newlength{\picwi}
\newcommand{\backskip}{\hspace{-2.5em}} % how much to skip back for an empty item?

% Set up some colors
\definecolor{myblue}{rgb}{0.14,0.11,0.49}
\definecolor{myred}{rgb}{0.74,0.1,0.05}
\definecolor{mygreen}{rgb}{0.,0.52,0.32}
\definecolor{myyellow}{rgb}{0.96,0.92,0.13}
\definecolor{myorange}{rgb}{0.7,0.41,0.1}
\definecolor{mypurple}{rgb}{0.51,0.02,.8}
\definecolor{mygray}{rgb}{0.6,0.6,0.6}

\newcommand{\myblue}[1]{\textcolor{myblue}{#1}}
\newcommand{\myred}[1]{\textcolor{myred}{#1}}
\newcommand{\mygreen}[1]{\textcolor{mygreen}{#1}}
\newcommand{\myorange}[1]{\textcolor{myorange}{#1}}
\newcommand{\myyellow}[1]{\textcolor{myellow}{#1}}
\newcommand{\mypurple}[1]{\textcolor{mypurple}{#1}}
\newcommand{\mygray}[1]{\textcolor{mygray}{#1}}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]

% Stlyle stuff
% notes are for students , \notes with \mmp{} are for me

\newcommand{\comment}[1]{}
\newcommand{\mmp}[1]{\emph MMP: {#1}}
\newcommand{\mydef}[1]{\myred{\bf {#1}}}
\newcommand{\myemph}[1]{\mygreen{ {#1}}}
\newcommand{\mycode}[1]{\myblue{\tt {#1}}}
\newcommand{\myexe}[1]{{\small \mypurple{Exercise} {#1}}}

\newcommand{\reading}[2]{{\small \myemph{{\bf Reading} CRLS:} {#1}, \myemph{Python APPB4AWD} {#2}}}


\begin{document}
\begin{Large}
\centerline{Math 285}
\centerline{Lecture Notes}  % lecture number here
\centerline{\bf }       % lecture title here
\centerline{}      %date here
\end{Large}

\section{September 6th}

We will start with a review of calculus, recast in into differential forms.

\begin{definition}
Let $U \in \mathbb{R}^n$ be an open set. For a function $f: U \to \mathbb{R}$, we say $f \in C^k_p$ at a point $p$ if all partial derivatives of $f$ with order $\leq k$ exist and are continuous at $k$.
\end{definition}

Example: $C^0(\mathbb{R})$ describes functions that are at least continuous over the real numbers.
In our setting, we will usually concern ourselves with functions that belong to $C^{\infty}$, where $C^\infty = \cap_{i=0}^\infty C^{i}$

\begin{definition}
Let $U \subseteq \mathbb{R}^n$, and let $f: U \to \mathbb{R}$. We call $f$ analytic at a point $p \in U$ if it agrees with its Taylor’s series at $p$ in some neighborhood of $p$.
\end{definition}

We notice that because taking derivatives is linear, that is, we can differentiate term by term, that if $f$ is analytic, then $f \in C^\infty$. However, the converse need not be true:

Consider:

$$ f = \begin{cases} e^{-1/x} & \text{ when } x > 0  \\ 0 & \text{ else } \end{cases} $$

Without too much work, we see that this function is continuous. Moreover, the derivative of $e^{-1/x}$ is equal to $x^{-2}e^{-1/x} = x^{-2}f$. Taking the limit as $x \to 0$, and using L’H\^opital’s rule where necessary, we can see this goes to 0. Alternatively, we can look at:

$$ f’(0) = \lim_{x \to 0} \frac{f(x) - f(0)}{x}$$

with reasonable usage of L’H\^opital’s.

The upshot is that, inductively, we may show that $f^{(k)}(0) = 0$, and thus, at the point $x = 0$, the Taylor series for $f$ is identically 0. However, in no neighborhood of 0, is $f(x)$ identically 0. Thus, $f$ is not analytic. However, via computation, we see that $f \in C^{\infty}$. So, $C^\infty \nRightarrow$ analytic.

Another way to see this concept, is if we think about Taylor’s Series up to $k$-th order. This is just a Taylor series truncated at the $k$-th term, with a remainder term $R_{k+1}$. Then, in such a view, $f$ is analytic at a point $p \iff \lim_{k \to \infty} R_k = 0$.

\begin{definition}
Let $U \in \mathbb{R}^n$ be a set, and $p \in U$. We call $U$ star-shaped with respect to $p$ if, for all $q \in U$, that the line segment $\overline{pq} \subset U$.
\end{definition}

This motivates the hypotheses for Taylor’s Theorem with a remainder term:

\begin{theorem}
Let $U \subset \mathbb{R}^n$ be a star-shaped open set with respect to a point $p \in U$. Let $f: U \to \mathbb{R}$. If $f \in C^\infty$, then there exist $g_1,...,g_n \in C^\infty$ such that

$$ f(x) = f(p) + \sum_{i=1}^n g_i(x) (x^i - p^i) \text{ with } g_i(p) = \frac{\partial f}{\partial x^i}(p) $$
\end{theorem}

Proof: Let $y \in U$, and let $x \in \overline{py} \subseteq U$. Taking a parametrization of $\overline{py}: x(t) = p + t(y-p) $ where the $i$-th component is given by $x^i(t) = p^i + t(y^i - p^i)$.

Now, consider $f(y) - f(p) = f(x(1)) - f(x(0))$. Using the fundamental theorem:

$$ f(x(1)) - f(x(0)) = \int_0^1 \frac{d}{dt} f(x(t)) dt  = \int_0^1 \sum \frac{\partial f}{\partial x^i} (x(t)) \frac{dx^i}{dt} dt = $$

$$ \sum_i  \int_0^1 \frac{\partial f}{\partial x^i} (p + t(y-p)) (y^i - p^i) dt =   \sum_i  \left(\int_0^1 \frac{\partial f}{\partial x^i} (p + t(y-p))dt \right) (y^i - p^i) $$

Thus, we identify: $$g_i(y) = \int_0^1 \frac{\partial f}{\partial x^i} (p + t(y-p))dt$$

It should be clear that:

$$g_i(p) =  \int_0^1 \frac{\partial f}{\partial x^i} (p + t(p-p))dt = \int_0^1 \frac{\partial f}{\partial x^i}(p)dt =  \frac{\partial f}{\partial x^i}(p) $$

as $ \frac{\partial f}{\partial x^i}(p)$ is not a function of $t$.

Further, by an application of the dominated convergence theorem:

$$ \frac{\partial}{\partial y^j} g_i(y) = \frac{\partial}{\partial y^j} \left( \int_0^1 \frac{\partial f}{\partial x^i} (p + t(p-p))dt\right) = $$
$$ \int_0^1 \frac{\partial}{\partial y^j} \frac{\partial f}{\partial x^i} (p + t(p-p))dt $$

which exists and is continuous because $f \in C^\infty$. Thus, $g_i \in C^\infty$

\section{September 11th}

We want to reformulate the concept of a tangent vector in a coordinate-free way, because we should not need to immerse our manifold in an ambient Euclidean space.

Note that we will use parentheses for a point, and angle brackets for a vectors.

Recall that for a surface traced out in $\mathbb{R}^3$ by some function $M: f(x^1, x^2, x^3) = 0$, we can say that the tangent space is:

$$ T_p(M)  = \{ v_p \in T_p(\mathbb{R}^3) : \nabla f(p) \cdot v_p = 0 \} $$

But this depends on the space we’re immersed in.

To move towards a coordinate independent description, we instead look at the directional derivative.

\begin{definition}
If $v_p \in T_p(U)$ and $f \in C^\infty(U)$, then the directional derivative of $f$ in the direction of $v_p$ at the point $p$ is denoted by $D_{v_p} f$.

Explicitly, we can describe this as a cross section $f(p + tv)$, and thus:

$$ D_{v_p} = \frac{d}{dt} \bigg|_{t=0} = \sum_i \frac{\partial }{\partial x^i}\bigg|_{x = (p + tv_p)} \frac{dx^i}{dt} \bigg|_{t=0} = \sum_i \frac{\partial }{\partial x^i}(p) v^i= \sum_i v_i \frac{\partial}{\partial x^i} \bigg|_{p}  $$ 
\end{definition}

This leads to the concept of the germs of a function:

\begin{definition}
Let $(f, U)$ denote a $C^\infty$ function and its domain: $f: U \to \mathbb{R}$. Fix a $p \in U$.

We say that $(f, U) \sim (g, V)$ if there exists $W \subset U \cap V$ such that $p \in W$, and that restricted to $W$, $f = g$. We denote these equivalence classes as $[(f,U)]$ and call these the germs of functions.

Further, we denote the set of equivalence classes at $p$ as $C_p^\infty$.

\end{definition}

With some work, we can show that due to our equivalence being on some neighborhood of $p$, and the derivative being a local characteristic, that we may apply the directional derivative as:

$$ D_{v_p}: C^\infty_p \to \mathbb{R}$$

Without too much trouble, we can see that there is a algebra of germs over $\mathbb{R}$ with the following operations:

$$ \begin{cases} [(f,U)] + [(g,V)] = [(f+g, U \cap V)] \\ [(f,U)] * [(g,V)] = [(f*g, U \cap V)] \\ \lambda[(f,U)] = [(\lambda f, U )] \end{cases} $$

\begin{proposition}
Let $D_{v_p} : C_p^\infty \to \mathbb{R}$.

(i) $D_{v_p}$ is $\mathbb{R}$-linear.

(ii) $D_{v_p}$ follows a Leibniz rule, that is: $D_{v_p}(fg) = D_{v_p}(f) g(p) + f(p) D_{v_p}(g) $


\end{proposition}

\begin{definition}
Let $D: C^\infty_p \to \mathbb{R}$. If $D$ satisfies (i) and (ii) from Proposition 2.1, then we call it a derivation at $p$, or equivalently, a point-derivation of $C^\infty_p$. 
\end{definition}

\begin{definition}
We denote the set of point-derivations of $C_p^\infty$ as $\mathcal{D}_p(\mathbb{R}^n)$.
\end{definition}

Note that $\mathcal{D}_p(\mathbb{R}^n)$ is closed under addition and scalar multiplication, but not under multiplication. Thus, this forms a vector space, but not an algebra.

So, now we can recast our tangent space.

\begin{theorem}

The map defined by: 

$$ \phi: T_p(\mathbb{R}^n) \to \mathcal{D}_p(\mathbb{R}) $$

such that $v_p \mapsto D_{v_p}$

is a linear isomorphism of vector spaces.

\end{theorem}

\begin{proof}

Suppose $D_{v_p}$ is the 0 operator. By definition:

$$ \varphi(v_p) = D_{v_p} = \sum_{i} v^i \frac{\partial}{\partial x^i} \bigg|_{p} $$

Since this is true for all functions, it is in particular true for the function $f = x^j$. Of course then:

$$ D_{v_p} (f) = \sum_{i} v^i \frac{\partial}{\partial x^i} \bigg|_{p} (f) = v^j_p = 0 $$

Since the choice of $x^j$ was arbitrary, this may be performed for each $x^j$. Thus, $v^i_p = 0$ for all $i$, and thus $v_p = 0$.

Now, let $D \in \mathcal{D}_p(\mathbb{R}^n)$ be an arbitrary point-derivation.

Define $D_{v_p} = \sum v^i \frac{\partial}{\partial x^i} \bigg|_p$ where $v^j =  D(x^j)$.

We claim that for an arbitrary $f \in C^\infty_p$, that $D f = D_{v_p} f$.

Using Theorem 1.1 (Taylor’s theorem with Remainder), we expand $f$ as:

$$ f(x) = f(p) + \sum g_i(x) (x^i - p^i) : g_i(p) =  \frac{\partial f}{\partial x^i}(p) $$

So, computing:

$$ D(f)  = D(f(p) + \sum g_i(x) (x^i - p^i)) = 0 + \sum D(g_i(x) (x^i - p^i)) = $$

$$\sum D(g_i) (p^i - p^i) + g_i(p) D(x^i - p^i) = \sum \frac{\partial f}{\partial x^i}(p) D(x^i) $$

We notice that this is exactly the same form as $\sum v^i \frac{\partial}{\partial x^i} \bigg|_p$ due to our identification of $v^i = D(x^i)$. Thus, $D = D_{v_p}$ on all $f$.
\end{proof}

Note: we will notate in the future as $e_{i,p} = \frac{\partial}{\partial x^i} \bigg|_p$ from now on. 

Because we have a bijection, we can establish the following definition:

\begin{definition}
$T_p(U)$ is the set of point derivations of $C^\infty_p$.
\end{definition}

\begin{definition}
Let $X: U \to \coprod_{p \in U} T_p (U)$. If $X_p \in T_p(U)$, we call such a function a vector field, where we use $\coprod$ to remind ourselves that the tangent spaces are disjoint.
\end{definition}

Unpacking the definition a little bit, if $X_p \in T_p(U)$ then:

$$ X_p = \sum a^i(p) \frac{\partial}{\partial x^i} \bigg|_p$$

Denoting $a^i: U \to \mathbb{R}$ as $p \mapsto a^i(p)$, then we have that:

$$ X = \sum a^i \frac{\partial}{\partial x^i}$$

\begin{definition}
Let $X$ be a vector field as above. We say that $X \in C^\infty$ if each $a^i: U \to \mathbb{R}$ is $C^\infty$.
\end{definition}

Notation: The set of $C^\infty$ vector spaces on $U$ is an $\mathbb{R}$-vector space. We denote this by $\mathcal{X}(U)$.

\section{September 13th}

Multilinear algebra:

Recall some easy examples.

The dot product $\langle u, v \rangle$ is a bilinear function that takes $V \times V \to k$.

Dual spaces:

Note that we like this point of view because we can always multiply functions, but we may not admit a multiplication on vectors. Cross products need not exist.

\begin{definition}
A covector of a vector space $V$ is a linear function $f: V \to \mathbb{R}$.
\end{definition}

\begin{definition}
The dual space of $V$ is the set of all covectors of $V$. We denote this as $V^*$.
\end{definition}

\begin{theorem}
Let $V$ be a vector space with basis $\{ e_i \}_{I=1}^n$, and we have $\alpha^i \in V^*$, where $\alpha^i(e_j) = \delta_i^j$. Then, $\{ \alpha^i \}$ form a basis for $V^*$. We call this the dual basis to $\{ e_j \}$.
\end{theorem}

\begin{proof}
Suppose $\sum_{I=1}^n c_i \alpha^I = 0$. Since these are functions on $V$, consider their action on $e_j$. Then, of course, we find that $c_j = 0$. Repeating this argument, this tells us that $c_i = 0$ for all $I$. In a similar fashion, if we assume $f$ to be a functional, its action is completely determined on the basis vectors. Then, we may construct g such that $g = \sum_{I=1}^n d_i \alpha^I$. And we notice $f-g = 0$ everywhere, so $f = g$.
\end{proof}

\begin{corollary}
The dimension of the dual space is the dimension of the vector space.
\end{corollary}

\begin{definition}
Let $f$ is a function, k-linear on $V$. That is, a function from $V \times V ... \times V \to \mathbb{R}$, linear in each argument. We call this object a $k$-tensor on $V$. Further, we call $f$ symmetric if, for any permutation of the arguments, $f$ is constant. $$ f(v_{\sigma(1)}, ..., v_{\sigma(n)}) = f(v_1,...,v_n)$$ for any $\sigma \in S_n$. Call $f$ alternative if, instead:
 $$ f(v_{\sigma(1)}, ..., v_{\sigma(n)}) = \text{sgn}(\sigma)f(v_1,...,v_n)$$
\end{definition}

\begin{definition}
Let $\sigma \in S_n$. We say that: 

$$ \text{sgn}(\sigma) = \begin{cases} 1 \text{ if } \sigma \text{ can be expressed as a product of an even number of transpositions} \\ -1 \text{ if } \sigma \text{ can be expressed as a product of an odd number of transpositions} \end{cases} $$

Equivalently, we can count the inversions.
\end{definition}

It turns out, that the symmetric tensors will not be terribly interesting for this course, but the alternating ones will be.

\begin{definition}
Denote the $k$-tensors on $V$ as the set $L_k(V)$.
Denote the alternating $k$-tensors on $V$ as the set $A_k(V)$
\end{definition}

\begin{definition}
Let $f \in L_k(V), g \in L_l(V)$. Denote the tensor product of $f$ and $g$ as $f \otimes g \in L_{k+k}(V)$, where:

$$ f \otimes g (v_1,...,v_{k+l}) = f(v_1,...,v_k)g(v_{k+1},...,v_{k+l}) $$

Note that this is not commutative, but it is associative.
\end{definition}

\begin{definition}
Let $f \in A_k(V), g \in A_l(V)$. We denote the wedge product of $f$ and $g$ as $f \wedge g$, computed as:
$$ f \wedge g (v_1,...v_{k+l}) = \frac{1}{k!l!} \sum_{\sigma \in S_{k+l}} \text{sgn}(\sigma)
f(v_{\sigma(1)},...,v_{\sigma(k)})g(v_{\sigma(k+1)},...,v_{\sigma(k+l)}) $$
\end{definition}

\begin{definition}
Let $\sigma \in S_{k+l}$. If $\sigma(1) < ... < \sigma(k)$ and $\sigma(k+1) < ... < \sigma(k+l)$, we call $\sigma$ a $(k,l)$ shuffle. Note that we have $\binom{k+l}{k}$
\end{definition}

We notice that instead of summing over all permutations of $S_n$, we may simply define the wedge product over $(k,l)$ shuffles, and drop the fraction in front.

\begin{definition}
Let $\sigma\in S_k$, and let $f \in L_k(V)$. Then, we denote the permutation of the arguments as $\sigma f (v_1,..,v_k) = f(v_{\sigma(1)},...,v_{\sigma(k)})$.
\end{definition}

\begin{definition}
Let $f \in L_k(V)$. We call the function:
$$ A(f) = \sum_{\sigma \in S_k} \text{sgn} (\sigma) (\sigma f)$$

the alternator of $f$.

\end{definition}

\begin{theorem}
For $f \in L_k(V)$, $A(k)$ is alternating.
\end{theorem}

\begin{proof}
Let $\tau \in S_n$. Then, we have that:
$$ \tau A(f) = \tau \sum_{\sigma}\text{sgn} (\sigma) (\sigma f) = \sum_{\sigma} \text{sgn} (\sigma) (\tau \sigma f) = \sum_{\sigma} \text{sgn} (\sigma) (\tau \sigma f) = \text{sgn}(\tau) \sum_\sigma \text{sgn}(\tau \sigma) (\tau \sigma) f $$

But, we see that summing over $\sigma$ is equivalent to summing over $\tau \sigma$ because it’s just a translation of the group. So, this is equal to $\text{sgn}(\tau) A(f)$, and we’re done.
\end{proof}

Thus, we notice that we can also write the wedge product:

$$ f \otimes g = \frac{1}{k! l!} A(f \otimes g)$$

Remark: The wedge product has the following properties:
(i) $f \wedge g = (-1)^{d_f d_g} g \wedge f$ where $d_f$ is the degree of $f$ and same for $g$.
(ii) It is associative, $(f \wedge g) \wedge h = f \wedge(g \wedge h)$.
(iii) $f \wedge g \wedge h = \frac{1}{k! l! m!} A(f \otimes g \otimes h)$.
(iv) For $\alpha^i \in V^*$, $(\alpha^1 \wedge...\wedge \alpha^k) (v_1,...,v_k) =
\det[\alpha^i(v_j)]$

\section{September 18th}

Differential forms on $\mathbb{R}^n$

Recall that a covector is a linear map from $V \to \mathbb{R}$, 1-tensor, alternating 1-tensor.

Analogously then, a covector field on an open set $U \subseteq \mathbb{R}$ assigns a covector to each point $u \in U$.

An alternative name for a covector field is a 1-form. 

Differentials of $f$:

\begin{definition}
For $f \in C^\infty(U)$, define the 1-form $df$ on $U$ via, for $p \in U$:

$$(df)_p (X_p) = D_{X_p} f := X_p f $$

\end{definition}

\begin{example}
$$(dx^i)_p \left(\frac{\partial}{\partial x^j} \bigg|_p\right) = \frac{\partial}{\partial x^j} \bigg|_p x^i = \delta_j^i $$
\end{example}

We notice that $\left\{ \frac{\partial}{\partial x^i} \right\}$ is a basis, and the standard basis for $T_p(\mathbb{R}^n)$.

Therefore, $\{ dx^i \}$ is the dual basis for $(T_p \mathbb{R}^n)^v : = T_p^*(\mathbb{R}^n)$, the cotangent space at $p$. 

Then, of course, every covector $w_p$ at $p$ may be expressed in this basis as:

$$ w_p = \sum b_i(p) dx^i_p $$

Viewing $b_i$ as a function over $U$, then:

$$ w = \sum b_i dx^i $$

where $b_i : U \to \mathbb{R}$.

\begin{definition}
A 1-form $ w = \sum b_i dx^i $ is $C^\infty$ if all $b_i \in C^\infty$.
\end{definition}

\begin{example}

Consider $\{ x, y, z\} \subset \mathbb{R}^3$ as the standard coordinates. 

$dx$ is a 1-form on $\mathbb{R}^3$ such that

$$ dx\left( a\frac{\partial}{\partial x} + b\frac{\partial}{\partial y} + c\frac{\partial}{\partial z} \right) = a$$

that is, it extracts the $x$-coordinate of a vector.
\end{example}

If we consider extracting coordinates via the dual basis, it’s not hard to see that, for $f \in C^\infty$:

$$ df = \sum \frac{\partial f}{\partial x^i} dx^i $$

Extending to $k$ arguments, we have the following:

\begin{definition}
A $k$-form on $U$ is an alternating $k$-tensor $w_p$ defined at each point $p \in U$. 
\end{definition}

Recall the following theorem:

\begin{theorem}
If $\alpha^1,...,\alpha^n$ is a basis for $A_1(V)$, then a basis for $A_k(V)$ is $\alpha^{i_1} \wedge ... \wedge \alpha^{i_k}$ where $i_1 < .. < i_k$. 
\end{theorem}

\begin{example}
In $\mathbb{R}^3$:

A 0-form associates to each point a number, so is a linear function $f: U \to \mathbb{R}$. 

A 1-form looks like:

$$ w = f dx + gdy + h dz $$

A 2-form looks like:

$$ w = P dy \wedge dz + Q dz \wedge dx + R dx \wedge dy $$

A 3-form looks like:

$$ w = f dx \wedge dy \wedge dz $$

In general, we define a $k$-form on $U \subset \mathbb{R}^n$ as:

$$ w = \sum_{I} b_I dx^I$$ where $I$ is a multi-index such that $1 \leq i_1 < ... < i_k \leq n$.
\end{example} 

We can see that there exists a 1-1 correspondence between 1-forms/2-forms and vector fields.

We can also see that these exists a 1-1 correspondence between 0-forms/3-forms and functions.

Not too hard to see, we would just look at these as abstract vector fields.

Exterior Derivative:

Notationally, we will denote $\Omega^k(U)$ as the $C^\infty$ k-forms on $U$.

\begin{definition}
We define the exterior derivative as a map:

$$d: \Omega^k(U) \to \Omega^{k+1}(U)$$

where $d\left( \sum_{I} b_I dx^I \right) = \sum_I db_I \wedge dx^I = \sum_{I,j} \frac{\partial b_I}{\partial x^j} dx^j \wedge dx^I$
\end{definition}

\begin{proposition}
(i) $d$ is an antiderivation of degree $1$:

$$ d(\omega \wedge \tau) = (d\omega) \wedge \tau + (-1)^{\deg \omega} \omega \wedge d\tau $$

(ii) $d \cdot c = 0$.

(iii) If $f$ is a 0-form, then:

$$ df(X) = Xf $$
\end{proposition}

See proof in book.

Recall some basics from vector calculus.

The gradient of $f$ may be viewed as:

$$\nabla f = \begin{bmatrix} f_x \\ f_y \\ f_x  \end{bmatrix}$$

We say this corresponds to the following 1-form:

$$df = f_x dx + f_y dy + f_z dz $$

Similarly, for the curl:

$$ \nabla \times f = \begin{bmatrix} \frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \\ \frac{\partial}{\partial z} \end{bmatrix} \times \begin{bmatrix} P \\ Q \\ R \end{bmatrix} = \begin{bmatrix} R_y - Q_z \\ P_z - R_x \\ Q_x - P_y \end{bmatrix} $$

This corresponds to the following 2-form:

$$(R_y - Q_z) dy \wedge dz + (P_z - R_x) dz \wedge dx + (Q_x - P_y) dx \wedge dy = d(P dx + Q dy + R dz) $$

Without writing it out, we can look at the divergence of a vector field, and we notice that it corresponds to the following 2-form:

$$d(P dy \wedge dz + Q dz \wedge dx + R dx \wedge dy) $$


\end{document}