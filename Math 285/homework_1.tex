\documentclass[10pt]{article}
\setlength{\parskip}{0.25\baselineskip}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{float}
\usepackage{bbm}

\newcommand{\supp}{{\text{supp}}} 
\newcommand{\bv}{{\text{BV}}}
\newcommand{\ac}{{\text{AC}}}
\newcommand{\vol}{{\text{Vol}}}

\newenvironment{problem}[2][]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
 
\title{Homework \#1}
\author{Eric Tao\\
Math 285: Homework \#1}
\maketitle

\begin{problem}{Question 1}

Define $f: \mathbb{R} \to \mathbb{R}$ via:

$$ f(x) = \begin{cases} e^{-1/x^2} & \text{ for } x \neq 0 \\ 0 & \text{ for } x = 0\end{cases}$$

Using the definition of the derivative, prove that $f \in C^\infty(\mathbb{R})$, and that the derivatives $f^{(k)}(0)$ vanish for all $k \in \mathbb{N}$. 

\end{problem}

\begin{proof}[Solution]

Well, first, we restrict ourselves to $\mathbb{R} \setminus \{ 0 \}$. On this domain, $f(x) =  e^{-1/x^2}$, and, familiarly, we can see that

$$f'(x) = e^{-x^{-2}} \frac{d}{dx} (-x^{-2})=  e^{-x^{-2}} 2x^{-3} = 2x^{-3} f(x) $$

then, we identify:

$$f^{(2)}(x) = f'(x)2x^{-3} - 6x^{-4}f(x) = 2x^{-3} f(x)2x^{-3} -  6x^{-4}f(x) = f(x) (4x^{-6} - 6x^{-4})$$

More generally, we can see that:

$$f^{(n)}(x) = f(x) p_n\left(\frac{1}{x}\right)$$

for $p_n\left(1/x\right)$ a polynomial in $1/x$ because the derivative of a polynomial in $1/x$ is simply a polynomial, and the derivative of $f$ is $f'(x) =  2x^{-3} f(x)$. Explicitly:

$$f^{(n+1)}(x) = \frac{d}{dx} \left[f(x) p_n\left(\frac{1}{x}\right)\right] = f'(x) p_n\left(\frac{1}{x}\right) + f(x) p_n'\left(\frac{1}{x}\right) \frac{-1}{x^2} = f(x) \left[ 2x^{-3} p_n\left(\frac{1}{x}\right) - p_n'\left(\frac{1}{x}\right)x^{-2}\right] = f(x) p_{n+1}\left(\frac{1}{x}\right)$$

Further, we notice that by our identification:

$$  p_{n+1}\left(\frac{1}{x}\right) = 2x^{-3} p_n\left(\frac{1}{x}\right) - p_n'\left(\frac{1}{x}\right)x^{-2} $$

and that because $p_0(1/x) = 1$, that $\deg(p_n(y)) = \deg(p_{n-1}(y)) + 3 = 3n$, where we can say this because the degree of the first term in the recurrence is $\deg(p_n) + 3$ and the degree of the second term is $\deg(p_n') + 2 = \deg(p_n) -1 + 2 = \deg(p_n)+1$, so the leading term in the first term cannot vanish, and we have that the overall polynomial has degree $\deg(p_n) + 3$.

Now, let's examine the limit as $x \to 0$ of $f^{(n)}(x)$. Well:

$$\lim_{x\to 0} f^{(n)}(x) = \lim_{x\to 0} f(x) p_n\left( \frac{1}{x} \right)$$

Here, we rewrite this as examining two easier limits, by letting $y = 1/x$:

$$\begin{cases} \lim_{y \to \infty} e^{-y^2} p_n(y) \\  \lim_{y \to -\infty} e^{-y^2} p_n(y) \end{cases} $$

where of course, $p_n(y)$ is a polynomial in $y$, with non-negative powers. Well:

$$\lim_{y \to \infty} e^{-y^2} p_n(y)  = \lim_{y \to \infty}\frac{p_n(y)}{e^{y^2}} $$

Making the assumption that the leading coefficient of $p_n(y)$ is positive (if not, we pull out a negative sign and examine $-p_n(y)$), we see that the conditions for L'H\^opital's is met, as both numerator and denominator diverge to positive infinity.

Thus, we may take a derivative to find:

$$  \lim_{y \to \infty}\frac{p_n(y)}{e^{y^2}} =  \lim_{y \to \infty}\frac{p_n'(y)}{2ye^{y^2}} =  \lim_{y \to \infty}\frac{p_n'(y)/2y}{e^{y^2}}$$  


Well, if $p_n'(y)/2y$ has no positive powers, then we're done, as in that case, $\lim_{y \to \infty} p_n'(y)/2y = c$, at most a constant. Otherwise, if it does have a positive power, its limit remains $\infty$, so we may iteratively keep taking derivatives since $p_n$ has finite degree until this is true. Thus, because $\lim_{y \to\ infty} e^{y^2} = \infty$, we have that:

$$ \lim_{y \to \infty}\frac{p_n(y)}{e^{y^2}}= 0$$

The same argument works for $y \to -\infty$, with potentially examining $-p_n(y)$ if the leading term has odd degree. Thus, in terms of our original limit, we have that:

$$\lim_{x\to 0} f^{(n)}(x) = \lim_{x\to 0} f(x) p_n\left( \frac{1}{x} \right) = 0$$

So, we have that to remove the discontinuity, we would enforce that $ f^{(n)}(0) = 0$ for all $n$. Thus, derivatives exist on all of $\mathbb{R}$ continuously and smoothly for all $n$, as we may patch the discontinuity at $x = 0$. Thus, $f \in C^\infty(\mathbb{R})$.

%where we define $p^*_n(x) = p_n(1/x) * 1/x^{3n}$, that is, the polynomial with non-negative powers of $x$ after factoring out by the highest power of $1/x$.

%Well:

%$$ \lim_{x\to 0} \frac{f(x) p^*_n(x)}{x^{3n}} $$

%is a form where we may apply L'H\^opital's rule to, as $\lim_{x \to 0} f(x) p^*_n(x) = 0$ since $\lim_{x \to 0} f(x) = 0$ and $\lim{x \to 0} p^*_n(x) = a_0$, some constant term and because $\lim_{x \to 0} x^{3n} = 0$.

\end{proof}

\begin{problem}{Question 2}

Let $\overline{0} = (0,0)$ be the origin in $\mathbb{R}^2$, and let $B(\overline{0},1)$ be the open unit disk centered at the origin. To find a diffeomorphism between $B(\overline{0},1)$ and $\mathbb{R}^2$, we identify $\mathbb{R}^2$ as $\{ (x,y,0) : x,y, \in \mathbb{R} \} \subseteq \mathbb{R}^3$, and introduce the lower open hemisphere:

$$ S = \{ (x,y,z) : x^2 + y^2 + (z-1)^2 = 1; z < 1 \} \subseteq \mathbb{R}^3 $$

as an intermediate space. Notice that the map:

$$f: B(\overline{0},1) \to S \text{ via } (a,b) \mapsto (a,b,1 - \sqrt{1 - a^2 - b^2}) $$

is a bijection.

(a) The stereographic projection $g: S \to \mathbb{R}^2$ through $(0,0,1)$ is the map that sends a point $(a,b,c) \in S$ to the point in the $xy$ plane given by the line through $(0,0,1)$ and $(a,b,c)$. Show that it is given by:

$$(a,b,c) \mapsto (u,v) = \left( \frac{a}{1-c}, \frac{b}{1-c} \right); c =1 - \sqrt{1 - a^2 -b^2} $$

and its inverse is given by:

$$(u,v) \mapsto \left( \frac{u}{w}, \frac{v}{w}, 1 - \frac{1}{w} \right); w = \sqrt{1 + u^2 + v^2} $$

(b) 

Define

$$h = g \circ f :  B(\overline{0},1) \to \mathbb{R}^2$$

Show that:

$$h(a,b) = \left( \frac{a}{\sqrt{1 - a^2 - b^2}}, \frac{b}{\sqrt{1 - a^2 - b^2}} \right)$$

Further, find a formula for $h^{-1}(u,v) = (f^{-1} \circ g^{-1})(u,v)$ and conclude that $h$ is a diffeomorphism. 

(c)

Generalize part (b) to $\mathbb{R}^n$.

\end{problem}

\begin{proof}[Solution]

(a)

Let $(a,b,c) \in S$, and consider the line through $(0,0,1)$ and $(a,b,c)$. In vector form, we may write this as:

$$ (x,y,z) = (0,0,1)t + (a,b,c) (1 - t)  \text{ s.t. } t \in \mathbb{R}$$

We wish to find the point at which $z = 0$. We find that for $z = 0$, then the value of $t$ is:

$$  0 = t_{z=0} + c(1 - t_{z=0}) \implies t_{z=0}  = \frac{c}{c - 1} $$

Now, we wish to see the values of $(x,y)$ at $t_{z=0}$. Substituting, we find that:

$$\begin{cases} x_{z=0} = a - at = a - a \frac{c}{c-1} = \frac{ac - a - ac}{c - 1} = \frac{-a}{ c- 1} = \frac{a}{1-c} \\ y_{z=0} = b - bt = b - b\frac{c}{c-1} = \frac{bc - b - bc}{c-1} = \frac{-b}{c-1} = \frac{b}{1-c}\end{cases} $$

Here, we notice that since we live on the lower open hemisphere, that we have that:

$$ a^2 + b^2 + (c-1)^2 = 1 \implies c = 1 - \sqrt{1 - a^2 - b^2} $$

where we choose the negative sign for the square root as we live on the lower hemisphere.

Thus, we have that the map from the hemisphere to the disk in the plane as being from:

$$ (a,b,c) \mapsto (u,v) = \left( \frac{a}{1-c}, \frac{b}{1-c} \right) \text{ where } c = 1 - \sqrt{1 - a^2 - b^2} $$

Now, we can examine the inverse. Let $(u,v)$ be a point on the plane identified with $z=0$, and consider the line that joins it and $(0,0,1)$.

We can parametrize the line with the form:

$$ (x,y,z) = (0,0,1)(1-t) + (u,v,0)t$$

Now, we need to intersect this with the sphere. We have that:

$$ \begin{cases} x = ut \\ y = vt \\ z = (1-t) \\ \end{cases}$$

So, substituting, we have that:

$$ u^2 t^2 + v^2 t^2 + (1-t - 1)^2 = 1 \implies t^2(u^2 + v^2 + 1) = 1 \implies t = \frac{1}{\sqrt{1 + u^2 + v^2}} $$

Thus, we have, as expected that:

$$ (u,v) \mapsto uw, vw, 1 - w  \text{ where } w = \frac{1}{\sqrt{1 + u^2 + v^2}} $$

Now, we need only show that these are inverses. Calling the first map $f_1$ and the second map $f_2$, we see that:

$$f_1 \circ f_2(u,v) = f_1( (uw,vw,1-w)) = \left(\frac{uw}{1 -(1-w)}, \frac{vw}{1 - (1-w)} \right) = (u,v) $$

and that:

$$ f_2 \circ f_1((a,b,c)) = f_2\left(\frac{a}{1-c}, \frac{b}{1-c}\right) $$

Computing first the value of $w$, we find that:

$$w = \frac{1}{\sqrt{1 + \frac{a^2}{(1-c)^2} + \frac{b^2}{(1-c)^2}}} = \frac{(1-c)}{\sqrt{(1-c)^2 +a^2 + b^2}} = (1-c) $$

where we use the fact that $(a,b,c)$ are points on the sphere and replace $(1-c)^2 +a^2 + b^2 = 1$.

Thus:

$$  f_2\left(\frac{a}{1-c}, \frac{b}{1-c}\right) = \left(\frac{a}{1-c} (1-c),  \frac{b}{1-c} (1-c), 1 - (1-c) \right) = (a,b,c) $$

Thus, we have that these maps act as inverses as well.

(b)

As instructed, we define $h = g \circ f : B(\overline{0}, 1) \to \mathbb{R}^2$.

Let’s consider the action of $h$ on a point $(a,b) \in B(\overline{0}, 1)$:

$$h((a,b)) = g \circ f ((a,b)) = g(f(a,b)) = g(  (a,b,1 - \sqrt{1 - a^2 - b^2}) ) = \left( \frac{a}{\sqrt{1-a^2 - b^2}}, \frac{b}{\sqrt{1 - a^2 - b^2} } \right)$$

Next, let’s consider such an inverse. Firstly, we notice that the inverse to $f$ acts like:

$$f^{-1} (a,b,c) = (a,b)$$

Then, we have that, for a point $(u,v) \in \mathbb{R}^2$:

$$ h^{-1}(u,v) = (f^{-1} \circ g^{-1})(u,v) = f^{-1} ( uw, vw, 1 - w ) = (uw, vw) = \left(\frac{u}{\sqrt{1 + u^2 + v^2}}, \frac{v}{\sqrt{1 + u^2 + v^2}} \right)$$

without writing it out, it should be clear that composing these maps will show that these are inverses. Then, we should just check that the inverse actually brings points to the disk. Indeed:

$$\left| \left(\frac{u}{\sqrt{1 + u^2 + v^2}}, \frac{v}{\sqrt{1 + u^2 + v^2}} \right) \right|^2 = \frac{u^2}{1 + u^2 + v^2} + \frac{v^2} {1 + u^2 + v^2}  = \frac{u^2 + v^2}{1 + u^2 + v^2} < \frac{1 + u^2 + v^2}{ 1 + u^2 + v^2} = 1 $$

Then, indeed, $h^{-1}$ is a map to the open disk.

(c)

Generalizing this process, take $\overline{0}_n$ to be the origin in $\mathbb{R}^n$. We may take the open disk $B(\overline{0}_n, 1)$ to be the open unit disk centered on the origin, and take a diffeomorphism to the sphere in $n+1$ dimensions defined via:

$$ (a_1,...,a_n) \mapsto (a_1,...,a_n, 1 - \sqrt{1 - a^2 - b^2} $$

Then, in a similar fashion, we pass to $\mathbb{R}^n$ via a stereographic projection, taking:

$$(a_1,...,a_{n+1}) \mapsto (u,v) = \left( \frac{a_1}{1-c}, \frac{a_2}{1-c}, ..., \frac{a_n}{ 1-c } \right) \text{ where } c = 1 - \sqrt{ 1 - \sum_{i=1}^n a_i^2 } $$ 

Composing these maps, analogously to parts a,b, we find that these act as smooth bijections between the open disk in $\mathbb{R}^n$ and all of $\mathbb{R}^n$ and thus there exists a diffeomorphism relating them.

\end{proof}

\begin{problem}{Question 3}

Let $A$ be an algebra over a field $k$. Let $D_1, D_2$ be derivations of $A$. Show that $D_1 \circ D_2$ need not be a derivation, but $D_1 \circ D_2 - D_2 \circ D_1$ is.

\end{problem}

\begin{proof}[Solution]

Counterexample: Let $A = k[x]$, the polynomial ring over the field $k$. Let $D_1 = \frac{d}{dx}$, the formal derivative with respect to $x$, and let $D_2 = x\frac{d}{dx}$.

First, we check that these are actual derivations. We can see this by the action on $x^n$ for arbitrary $n,m$ that these are $k$ linear maps:

$$D_1: \begin{cases} \frac{d}{dx}(cx^n) = cn x^{n-1} = c \frac{d}{dx}(x^n) \\   \frac{d}{dx}(x^n + x^m) = nx^{n-1} + mx^{m-1} = \frac{d}{dx}x^n + \frac{d}{dx}x^m \end{cases} $$

$$D_2: \begin{cases} x\frac{d}{dx}(cx^n) = xcn x^{n-1} = cxn x^{n-1} = cx \frac{d}{dx}(x^n) \\   x\frac{d}{dx}(x^n + x^m) = xnx^{n-1} + xmx^{m-1} = x \frac{d}{dx} x^{n} + x \frac{d}{dx} x^m \end{cases} $$

Further, we can see that this follows the Leibniz rule:

$$ D_1(fg) = \frac{d}{dx} (fg) = \frac{d}{dx}(f) g + f \frac{d}{dx}(g) = D_1(f) g + fD_1(g) $$

$$D_2(fg) = x \frac{d}{dx}(fg) = x \left[  \frac{d}{dx}(f) g + f \frac{d}{dx}(g) \right ] = x \frac{d}{dx}(f) g + f x  \frac{d}{dx}(g) = D_2(f) g + fD_2(g) $$

However, consider $D_2 \circ D_1$ acting on $x^3 = x^2 * x$:

$$D_2 \circ D_1 (x^3) = D_2(\frac{d}{dx} x^3 ) = D_2(3x^2)  = 6x$$

On the other hand:

$$D_2 \circ D_1(x^2) * x + x^2D_2\circ D_1(x) = D_2(2x) * x + x^2 D_2(1) = 2x^2 + 0 = 2x^2 $$

where we've used the fact that $\frac{d}{dx}(1) = 0$ to conclude $D_2(1) = 0$.

Thus, even though $D_1, D_2$ are derivations, $D_2 \circ D_1$ need not be a derivation.

However, now consider $D_1 \circ D_2 - D_2 \circ D_1$ for generic derivations $D_1, D_2$.

Well:

$$ D_1 \circ D_2 - D_2 \circ D_1(fg) = D_1 \circ D_2(fg) - D_2 \circ D_1(fg) = D_1(D_2(f)g  + fD_2(g) ) - D_2(D_1(f)g  + fD_1(g)) = $$

$$ D_1 \circ D_2(f) g + D_2(f) D_1(g) + D_1(f) D_2(g) + f D_1 \circ D_2(g) - [ D_2 \circ D_1(f)g + D_1(f) D_2(g) + D_2(f) D_1(g) + fD_2 \circ D_1(g) ] = $$  

$$ D_1 \circ D_2(f) g + f D_1 \circ D_2(g) - D_2 \circ D_1(f)g - fD_2 \circ D_1(g) =  [D_1 \circ D_2(f)  -  D_2 \circ D_1(f)]g  + f[ D_1 \circ D_2(g)  - D_2 \circ D_1(g) ] = $$

$$ D_1 \circ D_2 - D_2 \circ D_1(f)g  + f D_1 \circ D_2 - D_2 \circ D_1(g) $$

\end{proof}

\begin{problem}{Question 4}

Let $U$ be a neighborhood of a point $p \in \mathbb{R}^n$, and let $X,Y$ be smooth vector fields on $U$. Define a function $Z_p : C_p^\infty \to \mathbb{R}$ via:

$$Z_p(f) = X_p(Yf) $$

(a)

Show that $Z_p$ does not satisfy the Leibniz rule, and is therefore not a derivation at $p$.

(b)

Show that $X_pY  - Y_pX$ satsifies the Leibniz rule on $C_p^\infty$. Hence, $[X,Y]_p := X_p Y - Y_pX$ is a tangent vector at $p$, and $[X,Y]$ is a vector field on $U$. Call this the Lie bracket of $X, Y$.

(c)

Let $X = \sum_i a^i \frac{\partial}{\partial x^i}, Y =  \sum_j b^j \frac{\partial}{\partial x^j}$. Find the coefficient $c_k$ in $[X,Y] = \sum_k c^k \frac{\partial}{\partial x^k}$.

(d) 

Show that if the vector fields $X,Y$ are smooth on $U$, then their Lie bracket is also a smooth vector field on $U$.
\end{problem}

\begin{proof}[Solution]

(a)

Let $f,g$ be arbitrary functions in $C_p^\infty$, and consider the expression:

$$ Z_p(fg) = X_p(Y(fg)) $$

Because vector fields act as derivations and satisfy the Leibniz rule, we have that $Y(fg)= (Yf)g + f(Yg)$. But here, we see that then, of course:

$$ Z_p(fg) = X_p(Y(fg)) = X_p((Yf)g + f(Yg)) = X_p((Yf)g) + X_p (f(Yg)) = $$
$$ (X_p(Yf))g(p) + (Yf)(p)(X_p(g)) + (X_p(f))(Yg)(p) + f(p)(X_p(Yg))$$

which needs not coincide with $Z_p(f)g + f(Z_p(g)) =  (X_p(Y(f)))g(p) + f(p)(X_p(Y(g)))$ unless $ (Yf)(p)(X_p(g)) + (X_p(f))(Yg)(p)  = 0$. 

Indeed, take $p = (1)$, $X = \langle 1 \rangle, Y = \langle x \rangle$, and consider $ f(x) = x^2 = x * x$.

We have that:

$$\begin{cases} Z_p(x^2) = X_p(Y(x^2)) = X_p (2x^2) = 4x \Big\rvert_{x=1} = 4 \\ Z_p(x) * x  \Big\rvert_{x=1} + x  \Big\rvert_{x=1} Z_p(x) =2 X_p(Y(x)) = 2 X_p(x) = 2  \end{cases}$$

Thus, $Z_p$ needs not satisfy the Leibniz rule, and may not be a derivation at $p$.

(b)

Let $f,g$ be arbitrary functions in $C_p^\infty$, and consider the expression:

 $$[X_pY  - Y_pX](fg) = X_pY(fg) - Y_pX(fg) = X_p(Yf)(g) +  X_p (f)(Yg) - Y_p(Xf)(g) - Y_p(f)(Xg) = $$

$$ (X_pYf)g(p) + (Yf)(p) X_p(g) + X_p(f) (Yg)(p) + f(p) (X_p(Yg)) - $$
$$(Y_p(Xf))g(p) -  (Xf)(p)(Y_p(g)) - Y_p(f) (Xg)(p) - f(p) (Y_p(Xg)) $$

Here, we recognize that $X_p(f) = (Xf)(p)$ pretty much by definition, since:

$$(Xf)(p) = \left(\sum a^i \frac{\partial}{\partial x^i} f\right)(p) = \sum a^i(p) \frac{\partial}{\partial x^i} f  \Big\rvert_{p} = X_p(f)$$

Then, we notice that:

$$\begin{cases} (Yf)(p) X_p(g) - Y_p(f) (Xg)(p) = 0 \\  X_p(f) (Yg)(p) -  (Xf)(p)(Y_p(g)) = 0 \end{cases}$$

and we see that:

$$[X_pY  - Y_pX](fg) = (X_pYf)g(p)+ f(p) (X_p(Yg)) -(Y_p(Xf))g(p) - f(p) (Y_p(Xg)) = [X_pY - Y_p X](f) g(p)  - f(p) [X_pY - Y_p X](g)$$

that is, it satisfies the Leibniz rule.

(c)

Because derivations act linearly, we may consider the action of the bracket on each $x_l$ individually:

$$ [X,Y] = (XY - YX)(x_l) =  \sum_i a^i \frac{\partial}{\partial x^i} \sum_j b^j \frac{\partial}{\partial x^j} (x_l) - \sum_j b^j \frac{\partial}{\partial x^j}   \sum_i a^i \frac{\partial}{\partial x^i}(x_l) = $$
$$   \sum_i a^i \frac{\partial}{\partial x^i} b^l -  \sum_j b^j \frac{\partial}{\partial x^j} a^l $$

Then, since we notice of course that the action looks like:

$$ \sum_k c^k \frac{\partial}{\partial x^k} x_l = c^l $$

We may identify 

$$c^l =  \sum_i a^i \frac{\partial}{\partial x^i} b^l -  \sum_j b^j \frac{\partial}{\partial x^j} a^l = \sum_i  a^i \frac{\partial}{\partial x^i} b^l - b^i \frac{\partial}{\partial x^i} a^l $$

(d)

We may assume that $X, Y$ are smooth vector fields on an open set $U$. From part (c), we have the identification that:

$$[X,Y] = \sum_k c^k \frac{\partial}{\partial x^k}$$

with 

$$c^l = \sum_i  a^i \frac{\partial}{\partial x^i} b^l - b^i \frac{\partial}{\partial x^i} a^l $$

Fixing some $l$, let's look at any individual $a^i \partial/\partial x^i (b^l)  $. Since $Y$ is smooth, then $b^l$ is a smooth function as well. Then, so is its partial derivative. Further, since $X$ is smooth, so is $a^i$, so this is a product of two smooth functions, which is itself smooth.

This same argument applies to the second term $b^i\partial/\partial x^i a^l $. Thus, each term in the summation is a difference of smooth functions, which is smooth, and a sum of smooth functions is also smooth. Therefore, $c^l$ is smooth.

Since the choice were arbitrary, this argument applies to every coefficient function, and thus the Lie bracket itself is smooth.


\end{proof}

\begin{problem}{Question 5}

Let $V$ be a vector space of dimension $n$, with basis $e_1,...,e_n$. Let $\alpha^1,...,\alpha^n$ be the dual basis for $V^*$. Show that a basis for the space of $k$-linear functions of $V$, $L_k(V)$ is $\{ \alpha^{i_1} \otimes ... \otimes \alpha^{i_k} \}$ for any multi-index $(i_1,...,i_k)$. In particular, conclude that $\dim(L_k(V)) = n^k$.  

\end{problem}

\begin{proof}[Solution]

First, we validate that this is well defined, that is, a sum of $\alpha^I = \alpha^{i_1} \otimes ... \otimes \alpha^{i_k}$ for $I = (i_1,...,i_k)$ is an actual $k$-linear function.

Well, it suffices to show that this is true for some fixed $\alpha^{i_1} \otimes ... \otimes \alpha^{i_k}$, as the sum of linear functions and the product by scalars are still linear functions.

So, let $v_1, v_2 \in V^k$, and assume that $(e_{i_1}\otimes ... \otimes e_{i_k}) * (\alpha^{i_1} \otimes ... \otimes \alpha^{i_k}) = m$, and that in the decomposition of $v_1, v_2$, that the coefficient of the basis vector $(e_{i_1}\otimes ... \otimes e_{i_k})$ is $a_1, a_2$, respectively. Well, we have then that:

$$ \alpha^{i_1} \otimes ... \otimes \alpha^{i_k} (b v_1 + c v_2) =  a_1 m  b + a_2 m c =b  \alpha^{i_1} \otimes ... \otimes \alpha^{i_k} ( v_1) + c \alpha^{i_1} \otimes ... \otimes \alpha^{i_k} ( v_2)$$

where we use the fact that for any $ (e_{j_1}\otimes ... \otimes e_{j_k})$ such that some $j_l \neq i_l$, then $  (e_{j_1}\otimes ... \otimes e_{j_k}) *  (\alpha^{i_1} \otimes ... \otimes \alpha^{i_k}) = 0$.

Next, we wish to show that this is linearly independent. Suppose we have a collection of $c_l \alpha^{I_l}$ such that

$$ \sum c_l \alpha^{I_l}(v) = 0$$

for all $ v \in V^k$. Well, since this is true for all $v$, fix some $\alpha^{I_j} =   \alpha^{j_1} \otimes ... \otimes \alpha^{j_k}$, and choose $v = e_{j_1} \otimes ... \otimes e_{j_k}$. We have then that:

$$ \sum c_l \alpha^{I_l}(e_{j_1} \otimes ... \otimes e_{j_k}) = c_j \alpha^{I_j} (e_{j_1} \otimes ... \otimes e_{j_k}) = 0$$

However, by construction, we must have that $\alpha^{I_j} (e_{j_1} \otimes ... \otimes e_{j_k}) \neq 0$, because the $\alpha^i$ are a dual basis for $V^*$, and if this were 0, then at least one $e_{j_i} * \alpha^{j_i} = 0$. Thus, $c_j = 0$. Repeating this argument for each $\alpha^{I_l}$, we find that $c_j = 0$ for all $j$, and thus we have linear independence.

Now, we need only show that this spans the space of $k$-linear functions.

Let $f$ be a $k$ linear function. Because $f$ is linear, we need only describe it on the basis vectors for $V^k$, and this will describe it for all of $V^k$. Fix some $(e_{i_1}\otimes ... \otimes e_{i_k})$. Suppose that $f((e_{i_1}\otimes ... \otimes e_{i_k}) = f(e_I) = a_I$. Then, we define $b_I = a_I/ (e_{i_1}\otimes ... \otimes e_{i_k}) * (\alpha^{i_1} \otimes ... \otimes \alpha^{i_k})$.

Repeat this construction for each multi-index $I$, and construct the function

$$ g = \sum_I b_I \alpha^I$$

We notice that this is a function that agrees with $f$ at each basis vector:

$$ (g-f)(e_J) = g(e_J) - f(e_J) = a_J - a_J = 0$$

and therefore, agrees with $f$ everywhere. Thus, the set of  $\{ \alpha^{i_1} \otimes ... \otimes \alpha^{i_k} \}$ over every multi-index $I$ both spans the set of $k$ linear functions and is linearly independent, therefore a basis.

It is clear then, that the number of basis vectors is simply the number of multi-indices, which is exactly $\underbrace{n*...*n}_{k \text{ times }} = n^k $, as we have put no constraints over the values of $(i_1,...,i_k)$ other than $1 \leq i_j \leq n$. Therefore, we conclude that $\dim(L_k(V)) = n^k$.  

\end{proof}

\begin{problem}{Question 6}

Let $V$ be a vector space. For $a,b \in \mathbb{R}, f \in A_k(V), g \in A_l(V)$, show that $af \wedge bg = (ab) f \wedge g$
\end{problem}

\begin{proof}[Solution]

Letting $v_1,...,v_{k+l}$ being vectors in our underlying vector space, with $f$ a multilinear function of $k$ arguments and $g$ with $l$ arguments, we recall that the wedge product is defined as:

$$ f \wedge g (v_1,...,v_{k+l}) = \frac{1}{k!l!} A(f \otimes g)(v_1,...,v_{k+l}) = $$
$$  \frac{1}{k!l!} \sum_{\sigma \in S_{k+l}} (\text{sgn} \sigma) f(v_{\sigma(1)},...,v_{\sigma(k)}) g(v_{\sigma(k+1)},...,v_{\sigma(k+l)}) $$

where we sum over the possible permutations of $k+l$ elements. Well, then:

$$af \wedge bg = \frac{1}{k!l!} \sum_{\sigma \in S_{k+l}} (\text{sgn} \sigma) (af)(v_{\sigma(1)},...,v_{\sigma(k)}) (bg)(v_{\sigma(k+1)},...,v_{\sigma(k+l)}) = $$

$$ (ab) \frac{1}{k!l!} \sum_{\sigma \in S_{k+l}} (\text{sgn} \sigma) f(v_{\sigma(1)},...,v_{\sigma(k)}) g(v_{\sigma(k+1)},...,v_{\sigma(k+l)}) = (ab) f \wedge g $$

where we apply the distributive property to pull $(ab)$ from each term in the summation.

\end{proof}

\begin{problem}{Question 7}

Suppose we have two sets of covectors on a vector space $V$: $\{ \beta^1,...,\beta^k \}, \{ \gamma^1,....,\gamma^k \}$. Further, suppose we have that they are related by:

$$ \beta^i  = \sum_{j=1}^k a_j^i \gamma^j$$ for each $1 \leq i \leq k$, such that the $a_j^i$ form the entries of a $k\times k$ matrix $A$. Show that:

$$ \beta^1 \wedge ... \wedge \beta^k = \det(A) \gamma^1 \wedge ... \wedge \gamma^k$$

\end{problem}

\begin{proof}[Solution]

First, since our domain is a vector space $V$, then we have that these are $1$-linear functions. Thus, by 3.30, we have that $\beta^l \wedge \beta^l = 0$ for any $1 \leq l \leq k$.

Now, substituting, we see that:

$$ \beta^1 \wedge ... \wedge \beta^k = \sum_{j=1}^k a_j^1 \gamma^j \wedge ... \wedge \sum_{j=1}^k a_j^k \gamma^j $$

Here, using the bilinearity, graded commutativity, and associativity of the wedge product, we can see without too much trouble that for each term where $\gamma^l$ appears more than once, then the sum goes to 0. A small example goes like, for $k = 4$:

$$ \gamma^1 \wedge \gamma^2 \wedge \gamma^3 \wedge \gamma^1 = -  \gamma^1 \wedge \gamma^2 \wedge \gamma^1 \wedge \gamma^3 =  \gamma^1 \wedge \gamma^1 \wedge \gamma^2 \wedge \gamma^3 = 0 \wedge \gamma^2 \wedge \gamma^3 = 0$$

Then, we rewrite our original sum to:

$$ \sum_{j=1}^k a_j^1 \gamma^j \wedge ... \wedge \sum_{j=1}^k a_j^k \gamma^j = \sum_{\sigma \in S_n}  a_{\sigma(1)}^1 \gamma^{\sigma(1)} \wedge ... \wedge a_{\sigma(k)}^k \gamma^{\sigma(1)} $$ 

as these are exactly the choices such that we pick exactly one $\gamma^i$ per entry in the wedge product.

Now, using the multilinearity, we may extract the coefficients, and we may use the fact that the wedge product is alternating to reorder our products to be of the form:

$$ \sum_{\sigma \in S_n}  a_{\sigma(1)}^1 \gamma^{\sigma(1)} \wedge ... \wedge a_{\sigma(k)}^k \gamma^{\sigma(1)} = \sum_{\sigma \in S_n} \prod_{i=1}^k a_{\sigma(i)}^i (\text{sgn} \sigma) \gamma^1 \wedge ... \gamma^k  $$

However, here, we notice that $ \sum_{\sigma \in S_n} \prod_{i=1}^k a_{\sigma(i)}^i (\text{sgn} \sigma)$ is exactly the Leibniz formula for determinants, where we identify $a_i^j$ as the entry in the $(i,j)$ entry of a matrix. (wlog, could choose $(j,i)$ since tranposition does not change the determinant). Thus, we have that, letting $A_{ij} = a_i^j$, that:

$$ \beta^1 \wedge ... \wedge \beta^k = \det(A) \gamma^1 \wedge ... \wedge \gamma^k$$

\end{proof}

\end{document}